{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Normal Neural Net and custom cnn good accuracy 65.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AIhx73NqhIiz",
        "h8EoQo2KusWV",
        "-hwPc6T0uVdG",
        "bFctlhrhvc1J"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Faisal-NSU/CSE465/blob/main/Custom%20CNN%20model%20with%20Melspectogram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip-W_ZQjhECj"
      },
      "source": [
        "# Utils\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AiLL6KlhJsU"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import sys\n",
        "\n",
        "\n",
        "def plot_images(images, nrows = None, ncols = None, figsize = None, ax = None, \n",
        "                axis_style = 'on', bgr2rgb = True):\n",
        "    '''\n",
        "    Plots a given list of images and returns axes.Axes object\n",
        "    \n",
        "    Parameters\n",
        "    -----------\n",
        "    images: list\n",
        "            A list of images to plot\n",
        "            \n",
        "    nrows: int\n",
        "           Number of rows to arrange images into\n",
        "    \n",
        "    ncols: int\n",
        "           Number of columns to arrange images into\n",
        "    \n",
        "    figsize: tuple\n",
        "             Plot size (width, height) in inches\n",
        "           \n",
        "    ax: axes.Axes object\n",
        "        The axis to plot the images on, new axis will be created if None\n",
        "        \n",
        "    axis_style: str\n",
        "                'off' if axis are not to be displayed\n",
        "    '''\n",
        "    N = len(images)\n",
        "    if not isinstance(images, (list, np.ndarray)):\n",
        "        raise AttributeError(\"The images parameter should be a list of images, \"\n",
        "                             \"if you want to plot a single image, pass it as a \"\n",
        "                             \"list of single image\")\n",
        "\n",
        "    # Setting nrows and ncols as per parameter input\n",
        "    if nrows is None:\n",
        "        if ncols is None:\n",
        "            nrows = N\n",
        "            ncols = 1\n",
        "        else:\n",
        "            nrows = int(np.ceil(N / ncols))\n",
        "    else:\n",
        "        if ncols is None:\n",
        "            ncols = int(np.ceil(N / nrows))\n",
        "    \n",
        "    if ax is None:\n",
        "        _, ax = plt.subplots(nrows, ncols, figsize = figsize)\n",
        "    \n",
        "    if len(images) == 1:\n",
        "        if bgr2rgb == True:\n",
        "            images[0] = cv2.cvtColor(images[0], cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "        ax.imshow(images[0])\n",
        "        ax.axis(axis_style)\n",
        "        \n",
        "        return ax\n",
        "    \n",
        "    else:\n",
        "        for i in range(nrows):\n",
        "            for j in range(ncols):\n",
        "                if (i * ncols + j) < N:\n",
        "                    img = images[i * ncols + j]\n",
        "                    \n",
        "                    if bgr2rgb == True:\n",
        "                            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                    \n",
        "                    # For this condition, ax is a 2d array else a 1d array\n",
        "                    if nrows >1 and ncols > 1: \n",
        "                        ax[i][j].imshow(img)\n",
        "                    \n",
        "                    else:\n",
        "                        ax[i + j].imshow(img)\n",
        "                \n",
        "                if nrows > 1 and ncols > 1:\n",
        "                    ax[i][j].axis(axis_style)\n",
        "                else:\n",
        "                    ax[i + j].axis(axis_style)\n",
        "        \n",
        "        return ax\n",
        "\n",
        "\n",
        "def drawProgressBar(current, total, string = '', barLen = 20):\n",
        "    '''\n",
        "    Draws a progress bar, something like [====>    ] 20%\n",
        "    \n",
        "    Parameters\n",
        "    ------------\n",
        "    current: int/float\n",
        "             Current progress\n",
        "    \n",
        "    total: int/float\n",
        "           The total from which the current progress is made\n",
        "             \n",
        "    string: str\n",
        "            Additional details to write along with progress\n",
        "    \n",
        "    barLen: int\n",
        "            Length of progress bar\n",
        "    '''\n",
        "    percent = current/total\n",
        "    arrow = \">\"\n",
        "    if percent == 1:\n",
        "        arrow = \"\"\n",
        "    # Carriage return, returns to the begining of line to owerwrite\n",
        "    sys.stdout.write(\"\\r\")\n",
        "    sys.stdout.write(\"Progress: [{:<{}}] {}/{}\".format(\"=\" * int(barLen * percent) + arrow, \n",
        "                                                         barLen, current, total) + string)\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "def get_fixed_audio_len(wav, sr, audio_len):\n",
        "    '''\n",
        "    Converts a time-series audio to a fixed length either by padding or trimming\n",
        "    \n",
        "    Parameters\n",
        "    -------------\n",
        "    wav: Audio time-series\n",
        "    \n",
        "    sr: Sample rate\n",
        "    \n",
        "    audio_len: The fixed audio length needed in seconds\n",
        "    '''\n",
        "    if wav.shape[0] < audio_len * sr:\n",
        "        wav = np.pad(wav, int(np.ceil((audio_len * sr - wav.shape[0])/2)), mode = 'reflect')\n",
        "    wav = wav[:audio_len * sr]\n",
        "    \n",
        "    return wav\n",
        "\n",
        "def get_melspectrogram_db(wav, sr, audio_len = 4, n_fft = 2048, hop_length = 512, \n",
        "                          n_mels = 128, fmin = 20, fmax = 8300, top_db = 80):\n",
        "    '''\n",
        "    Decomposes the audio sample into different frequencies using fourier transform \n",
        "    and converts frequencies to mel scale and amplitude to decibel scale.\n",
        "    \n",
        "    Parameters\n",
        "    -------------------\n",
        "    wav: Audio time-series\n",
        "    \n",
        "    sr: Sample rate\n",
        "    \n",
        "    audio_len: The fixed length of audio in seconds\n",
        "    \n",
        "    n_fft: Length of the Fast Fourier Transform window\n",
        "    \n",
        "    hop_length: Number of samples between successive frames\n",
        "    \n",
        "    n_mels: Number of mel filters, which make the height of spectrogram image\n",
        "    \n",
        "    fmin: Lowest frequency\n",
        "    \n",
        "    fmax: Heighest frequency\n",
        "    \n",
        "    top_db: Threashold of the decibel scale output\n",
        "    '''\n",
        "    wav = get_fixed_audio_len(wav, sr, audio_len)\n",
        "        \n",
        "    spec = librosa.feature.melspectrogram(wav, sr = sr, n_fft = n_fft, hop_length = hop_length, \n",
        "                                          n_mels = n_mels, fmin = fmin, fmax = fmax)\n",
        "    \n",
        "    spec = librosa.power_to_db(spec, top_db = top_db)\n",
        "    return spec\n",
        "\n",
        "def spec_to_image(spec):\n",
        "    '''\n",
        "    Converts the spectrogram to an image\n",
        "    \n",
        "    Parameters\n",
        "    -------------\n",
        "    spec: Spectrogram\n",
        "    '''\n",
        "    eps=1e-6\n",
        "    \n",
        "    # Z-score normalization\n",
        "    mean = spec.mean()\n",
        "    std = spec.std()\n",
        "    spec_norm = (spec - mean) / (std + eps)\n",
        "    spec_min, spec_max = spec_norm.min(), spec_norm.max()\n",
        "    \n",
        "    # Min-max scaling\n",
        "    spec_scaled = 255 * (spec_norm - spec_min) / (spec_max - spec_min)\n",
        "    spec_scaled = spec_scaled.astype(np.uint8)\n",
        "    \n",
        "    return spec_scaled"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIhx73NqhIiz"
      },
      "source": [
        "# Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wN-FHDThXLT"
      },
      "source": [
        "import IPython.display as ipd\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwks3biRjTPz"
      },
      "source": [
        "# Drive Mount\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSynufL5s4yR",
        "outputId": "998f21fc-00f8-4bc1-c3c7-3dd94e469809"
      },
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF5Zd8bZwzR3"
      },
      "source": [
        "# Training an Artificial Neural Network on time-series audio data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1-4S8AbAeKL7Tl_jlqT-7sA9sc8nmW2qa"
      ],
      "metadata": {
        "id": "FtV_ZY1p_R-f",
        "outputId": "c858e34c-355e-414e-b483-3c3a4083b150",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-4S8AbAeKL7Tl_jlqT-7sA9sc8nmW2qa\n",
            "To: /content/dataDic\n",
            "100% 2.47G/2.47G [00:32<00:00, 76.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load pickle\n",
        "import pickle\n",
        "filename = 'dataDic'\n",
        "infile = open(filename,'rb')\n",
        "dataDic = pickle.load(infile)\n",
        "infile.close()"
      ],
      "metadata": {
        "id": "j7qy32Y4qxPF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSpaBwHPHZmJ",
        "outputId": "f4d814d3-3a68-4797-85b6-6951d347f8dd"
      },
      "source": [
        "dataDic['train_time_series'].shape,dataDic['val_time_series'].shape,dataDic['test_time_series'].shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4900, 88200), (700, 88200), (1400, 88200))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1ykonyMEqdk"
      },
      "source": [
        "# Convert numpy arrays to torch tensors\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "train_time_series = torch.from_numpy(dataDic['train_time_series'])\n",
        "train_labels = torch.from_numpy(dataDic['train_labels']).long()\n",
        "\n",
        "val_time_series = torch.from_numpy(dataDic['val_time_series'])\n",
        "val_labels = torch.from_numpy(dataDic['val_labels']).long()\n",
        "\n",
        "test_time_series = torch.from_numpy(dataDic['test_time_series'])\n",
        "test_labels = torch.from_numpy(dataDic['test_labels']).long()\n",
        "\n",
        "# Create data loaders\n",
        "train_time_series = data_utils.TensorDataset(train_time_series, train_labels)\n",
        "train_loader = data_utils.DataLoader(train_time_series, batch_size = BATCH_SIZE, shuffle = False)\n",
        "\n",
        "val_time_series = data_utils.TensorDataset(val_time_series, val_labels)\n",
        "val_loader = data_utils.DataLoader(val_time_series, batch_size = BATCH_SIZE, shuffle = False)\n",
        "\n",
        "test_time_series = data_utils.TensorDataset(test_time_series, test_labels)\n",
        "test_loader = data_utils.DataLoader(test_time_series, batch_size = BATCH_SIZE, shuffle = False)\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes ={0: 'ANGRY',\n",
        " 1: 'DISGUST',\n",
        " 2: 'FEAR',\n",
        " 3: 'HAPPY',\n",
        " 4: 'NEUTRAL',\n",
        " 5: 'SAD',\n",
        " 6: 'SURPRISE'}"
      ],
      "metadata": {
        "id": "DFRPUx0FjcD_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eD1gTmGLH5GQ",
        "outputId": "2a8b9d68-763b-4495-81d9-c2a86d8a85d7"
      },
      "source": [
        "\n",
        "NUM_CLASSES = len(classes)\n",
        "\n",
        "N_FEATURES = train_time_series[0][0].shape[0]\n",
        "N_FEATURES"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88200"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iAofwVrSPrI"
      },
      "source": [
        "# Training a Convolutional neural network on spectrogram images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeVdfw3GST4Y"
      },
      "source": [
        "train_sr = 22050\n",
        "val_sr = 22050\n",
        "test_sr = 22050\n",
        "\n",
        "def get_spec_loader(audio_time_series, sr, batch_size, shuffle = False):\n",
        "    '''\n",
        "    Returns data loader of spectrogram images\n",
        "    \n",
        "    Parameters\n",
        "    ------------\n",
        "    audio_time_series: Tensor Dataset with wav, label iterables\n",
        "    \n",
        "    sr: Sample rate\n",
        "    \n",
        "    batch_size: The batch size of data loader\n",
        "    '''\n",
        "    audio_spec_img = []\n",
        "    labels = []\n",
        "    curr = 0\n",
        "    tot = len(audio_time_series)\n",
        "\n",
        "    for wav, label in audio_time_series:\n",
        "        spec_img = spec_to_image(get_melspectrogram_db(wav.numpy(), sr))\n",
        "        spec_img = np.expand_dims(spec_img, axis = 0)\n",
        "        audio_spec_img.append(spec_img)\n",
        "        labels.append(label)\n",
        "\n",
        "        curr += 1\n",
        "        drawProgressBar(curr, tot, barLen = 40)\n",
        "\n",
        "    audio_spec_img = torch.Tensor(audio_spec_img)\n",
        "    audio_spec_img = audio_spec_img / 255\n",
        "    \n",
        "    labels = torch.Tensor(labels).long()\n",
        "\n",
        "    audio_spec_img = data_utils.TensorDataset(audio_spec_img, labels)\n",
        "    audio_loader = data_utils.DataLoader(audio_spec_img, batch_size = batch_size, shuffle = shuffle)\n",
        "    \n",
        "    return audio_loader"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnbWUVyCSaWH",
        "outputId": "662316f1-010f-4ff4-e262-e01de411bd4d"
      },
      "source": [
        "# Getting the spectrogram image for each audio in train set\n",
        "import time\n",
        "start_time = time.time()\n",
        "train_loader = get_spec_loader(train_time_series, train_sr, BATCH_SIZE, shuffle = False)\n",
        "val_loader = get_spec_loader(val_time_series, val_sr, BATCH_SIZE, shuffle = False)\n",
        "test_loader = get_spec_loader(test_time_series, test_sr, BATCH_SIZE, shuffle = False)\n",
        "print(\"\\n--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: [========================================] 4900/4900"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: [========================================] 1400/1400\n",
            "--- 223.48922061920166 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = test_loader.dataset[0]\n",
        "print(x.shape)"
      ],
      "metadata": {
        "id": "TGgSG2vj7QbO",
        "outputId": "ee386388-1aa9-4078-fd32-1aba2687b87a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 128, 173])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlrpLW0wSxQm"
      },
      "source": [
        "## Model cnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4DKAzI_SqAP"
      },
      "source": [
        "import torch.nn as nn\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        \n",
        "        # Layer 1, Input shape (1, 128, 173) ->  Output shape (8, 62, 84)\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = (5, 6)), \n",
        "            nn.MaxPool2d(kernel_size = (2, 2)),\n",
        "            nn.ReLU(inplace=True), \n",
        "            nn.BatchNorm2d(8)            \n",
        "            )\n",
        "        \n",
        "        # Layer 2, Input shape (8, 62, 84) -> Output shape (16, 30, 41)\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = (3, 3)), \n",
        "            nn.MaxPool2d(kernel_size = (2, 2)),\n",
        "            nn.ReLU(inplace=True), \n",
        "            nn.BatchNorm2d(16)\n",
        "            ) \n",
        "        \n",
        "        # Layer 3, Input shape (16, 30, 41) -> Output shape (64, 10, 15)\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = (6, 7)), \n",
        "            nn.MaxPool2d(kernel_size = (2, 2)),\n",
        "            nn.ReLU(inplace=True), \n",
        "            nn.BatchNorm2d(32),   \n",
        "            nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = (6, 6)), \n",
        "            nn.MaxPool2d(kernel_size = (2, 2)),\n",
        "            nn.ReLU(inplace=True), \n",
        "            nn.BatchNorm2d(64)  \n",
        "            )\n",
        "        \n",
        "        # Fully Connected layer 1, Input features 64 * 10 * 15 -> Output features 512\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(1152, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(128, NUM_CLASSES)\n",
        "        )\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        \n",
        "        x = x.view(-1, self.num_flat_features(x))    \n",
        "        \n",
        "        logits = self.fc1(x)\n",
        "        predictions = self.softmax(logits)\n",
        "        return predictions\n",
        "    \n",
        "    \n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]\n",
        "        n_features = 1\n",
        "        for s in size:\n",
        "            n_features = n_features * s\n",
        "        \n",
        "        return n_features"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69uM_9H3TjSm"
      },
      "source": [
        "# Defining loss and optimizer\n",
        "NUM_CLASSES = len(classes)\n",
        "model = ConvNet().to(device)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "#step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)"
      ],
      "metadata": {
        "id": "gEhaMKMxvJIn"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer,lowest_loss):\n",
        "    model.train()\n",
        "    size = int(len(dataloader.dataset) / BATCH_SIZE)\n",
        "    for batch, (X,y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        \n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total = y.size(0)\n",
        "        _, predicted = torch.max(pred, dim = 1)\n",
        "        correct = (predicted == y).sum().item()\n",
        "        accuracy = correct / total\n",
        "\n",
        "        drawProgressBar((batch + 1), size, \n",
        "                              '\\t loss: {:.4f} \\t acc: {:.4f}'.format(round(loss.item(), 4), round(accuracy, 4)))\n",
        "        \n",
        "    if abs(lowest_loss - loss.item()) < THRESHOLD:\n",
        "        #early_stop_epoch += 1\n",
        "        print(' Loss did not decrease from ' + str(lowest_loss))\n",
        "    \n",
        "    else:\n",
        "        print(' Loss decreased from {:.4f} to {:.4f}, saving model.'.format(\n",
        "            round(lowest_loss, 4), round(loss.item(), 4)))\n",
        "        \n",
        "        lowest_loss = loss.item()\n",
        "        early_stop_epoch = 0\n",
        "        torch.save(model,'model.pth')\n",
        "        \n",
        "    acc_hist.append(accuracy)\n",
        "    loss_hist.append(loss.item())\n",
        "    return lowest_loss\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for (X,y) in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "-Bfb2uxcoy2I"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "EPOCHS = 50\n",
        "loss_hist = []\n",
        "acc_hist = []\n",
        "THRESHOLD = 0.001\n",
        "lowest_loss = np.inf\n",
        "start = time.time()\n",
        "for t in range(EPOCHS):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    lowest_loss = train_loop(train_loader, model, criterion, optimizer,lowest_loss)\n",
        "    test_loop(val_loader, model, criterion)\n",
        "final = (time.time() - start)/60\n",
        "print(f\"Done for all {EPOCHS} epochs in {math.ceil(final)} minutes\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bN0fMA_Xo_1l",
        "outputId": "8dbbd2d1-cdfa-4556-f31f-24ba11519839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Progress: [===>                ] 122/612\t loss: 2.1654 \t acc: 0.0000"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_single_epoch(model, dataloader, device):\n",
        "  correct = 0\n",
        "  size = len(dataloader.dataset)\n",
        "\n",
        "  model.eval()\n",
        "  for input,target in dataloader:\n",
        "        input, target = input.to(device), target.to(device)\n",
        "        # calculate loss\n",
        "        prediction = model(input)\n",
        "        correct += (prediction.argmax(1) == target).type(torch.float).sum().item()\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}% \\n\")"
      ],
      "metadata": {
        "id": "liW_rSOrpeEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_single_epoch(model,test_loader,device)"
      ],
      "metadata": {
        "id": "-IkPWG_BpjLW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}