{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Normal Neural Net and custom cnn good accuracy 65.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AIhx73NqhIiz",
        "h8EoQo2KusWV",
        "-hwPc6T0uVdG",
        "bFctlhrhvc1J"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Faisal-NSU/CSE465/blob/main/Making%20pickle%20obvi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip-W_ZQjhECj"
      },
      "source": [
        "# Utils\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AiLL6KlhJsU"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import sys\n",
        "\n",
        "\n",
        "def plot_images(images, nrows = None, ncols = None, figsize = None, ax = None, \n",
        "                axis_style = 'on', bgr2rgb = True):\n",
        "    '''\n",
        "    Plots a given list of images and returns axes.Axes object\n",
        "    \n",
        "    Parameters\n",
        "    -----------\n",
        "    images: list\n",
        "            A list of images to plot\n",
        "            \n",
        "    nrows: int\n",
        "           Number of rows to arrange images into\n",
        "    \n",
        "    ncols: int\n",
        "           Number of columns to arrange images into\n",
        "    \n",
        "    figsize: tuple\n",
        "             Plot size (width, height) in inches\n",
        "           \n",
        "    ax: axes.Axes object\n",
        "        The axis to plot the images on, new axis will be created if None\n",
        "        \n",
        "    axis_style: str\n",
        "                'off' if axis are not to be displayed\n",
        "    '''\n",
        "    N = len(images)\n",
        "    if not isinstance(images, (list, np.ndarray)):\n",
        "        raise AttributeError(\"The images parameter should be a list of images, \"\n",
        "                             \"if you want to plot a single image, pass it as a \"\n",
        "                             \"list of single image\")\n",
        "\n",
        "    # Setting nrows and ncols as per parameter input\n",
        "    if nrows is None:\n",
        "        if ncols is None:\n",
        "            nrows = N\n",
        "            ncols = 1\n",
        "        else:\n",
        "            nrows = int(np.ceil(N / ncols))\n",
        "    else:\n",
        "        if ncols is None:\n",
        "            ncols = int(np.ceil(N / nrows))\n",
        "    \n",
        "    if ax is None:\n",
        "        _, ax = plt.subplots(nrows, ncols, figsize = figsize)\n",
        "    \n",
        "    if len(images) == 1:\n",
        "        if bgr2rgb == True:\n",
        "            images[0] = cv2.cvtColor(images[0], cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "        ax.imshow(images[0])\n",
        "        ax.axis(axis_style)\n",
        "        \n",
        "        return ax\n",
        "    \n",
        "    else:\n",
        "        for i in range(nrows):\n",
        "            for j in range(ncols):\n",
        "                if (i * ncols + j) < N:\n",
        "                    img = images[i * ncols + j]\n",
        "                    \n",
        "                    if bgr2rgb == True:\n",
        "                            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                    \n",
        "                    # For this condition, ax is a 2d array else a 1d array\n",
        "                    if nrows >1 and ncols > 1: \n",
        "                        ax[i][j].imshow(img)\n",
        "                    \n",
        "                    else:\n",
        "                        ax[i + j].imshow(img)\n",
        "                \n",
        "                if nrows > 1 and ncols > 1:\n",
        "                    ax[i][j].axis(axis_style)\n",
        "                else:\n",
        "                    ax[i + j].axis(axis_style)\n",
        "        \n",
        "        return ax\n",
        "\n",
        "\n",
        "def drawProgressBar(current, total, string = '', barLen = 20):\n",
        "    '''\n",
        "    Draws a progress bar, something like [====>    ] 20%\n",
        "    \n",
        "    Parameters\n",
        "    ------------\n",
        "    current: int/float\n",
        "             Current progress\n",
        "    \n",
        "    total: int/float\n",
        "           The total from which the current progress is made\n",
        "             \n",
        "    string: str\n",
        "            Additional details to write along with progress\n",
        "    \n",
        "    barLen: int\n",
        "            Length of progress bar\n",
        "    '''\n",
        "    percent = current/total\n",
        "    arrow = \">\"\n",
        "    if percent == 1:\n",
        "        arrow = \"\"\n",
        "    # Carriage return, returns to the begining of line to owerwrite\n",
        "    sys.stdout.write(\"\\r\")\n",
        "    sys.stdout.write(\"Progress: [{:<{}}] {}/{}\".format(\"=\" * int(barLen * percent) + arrow, \n",
        "                                                         barLen, current, total) + string)\n",
        "    sys.stdout.flush()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIhx73NqhIiz"
      },
      "source": [
        "# Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wN-FHDThXLT"
      },
      "source": [
        "import IPython.display as ipd\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwks3biRjTPz"
      },
      "source": [
        "# Drive Mount\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1k-afEJdwz5Tf4-bsuKOJzP7xn-KQTlkM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqCp4GKwkjtn",
        "outputId": "a6959fd8-cd21-4d4f-e54c-fa684055143f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1k-afEJdwz5Tf4-bsuKOJzP7xn-KQTlkM\n",
            "To: /content/SUBESCO.zip\n",
            "100% 1.65G/1.65G [00:14<00:00, 110MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5zwz6XOjZO0"
      },
      "source": [
        "import zipfile\n",
        "dataset_directory = '/content/SUBESCO.zip'\n",
        "zip_ref = zipfile.ZipFile(dataset_directory, 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jSynufL5s4yR",
        "outputId": "785a8e06-9cdd-48f3-c9dd-a4f956c71880"
      },
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla K80'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXbAd2H9sJsp"
      },
      "source": [
        "# Getting Features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8EoQo2KusWV"
      },
      "source": [
        "## Define Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qnWmXbPsM21"
      },
      "source": [
        "def get_fixed_audio_len(wav, sr, audio_len):\n",
        "    '''\n",
        "    Converts a time-series audio to a fixed length either by padding or trimming\n",
        "    \n",
        "    Parameters\n",
        "    -------------\n",
        "    wav: Audio time-series\n",
        "    \n",
        "    sr: Sample rate\n",
        "    \n",
        "    audio_len: The fixed audio length needed in seconds\n",
        "    '''\n",
        "    if wav.shape[0] < audio_len * sr:\n",
        "        wav = np.pad(wav, int(np.ceil((audio_len * sr - wav.shape[0])/2)), mode = 'reflect')\n",
        "    wav = wav[:audio_len * sr]\n",
        "    \n",
        "    return wav\n",
        "\n",
        "def get_melspectrogram_db(wav, sr, audio_len = 4, n_fft = 2048, hop_length = 512, \n",
        "                          n_mels = 128, fmin = 20, fmax = 8300, top_db = 80):\n",
        "    '''\n",
        "    Decomposes the audio sample into different frequencies using fourier transform \n",
        "    and converts frequencies to mel scale and amplitude to decibel scale.\n",
        "    \n",
        "    Parameters\n",
        "    -------------------\n",
        "    wav: Audio time-series\n",
        "    \n",
        "    sr: Sample rate\n",
        "    \n",
        "    audio_len: The fixed length of audio in seconds\n",
        "    \n",
        "    n_fft: Length of the Fast Fourier Transform window\n",
        "    \n",
        "    hop_length: Number of samples between successive frames\n",
        "    \n",
        "    n_mels: Number of mel filters, which make the height of spectrogram image\n",
        "    \n",
        "    fmin: Lowest frequency\n",
        "    \n",
        "    fmax: Heighest frequency\n",
        "    \n",
        "    top_db: Threashold of the decibel scale output\n",
        "    '''\n",
        "    wav = get_fixed_audio_len(wav, sr, audio_len)\n",
        "        \n",
        "    spec = librosa.feature.melspectrogram(wav, sr = sr, n_fft = n_fft, hop_length = hop_length, \n",
        "                                          n_mels = n_mels, fmin = fmin, fmax = fmax)\n",
        "    \n",
        "    spec = librosa.power_to_db(spec, top_db = top_db)\n",
        "    return spec\n",
        "\n",
        "def spec_to_image(spec):\n",
        "    '''\n",
        "    Converts the spectrogram to an image\n",
        "    \n",
        "    Parameters\n",
        "    -------------\n",
        "    spec: Spectrogram\n",
        "    '''\n",
        "    eps=1e-6\n",
        "    \n",
        "    # Z-score normalization\n",
        "    mean = spec.mean()\n",
        "    std = spec.std()\n",
        "    spec_norm = (spec - mean) / (std + eps)\n",
        "    spec_min, spec_max = spec_norm.min(), spec_norm.max()\n",
        "    \n",
        "    # Min-max scaling\n",
        "    spec_scaled = 255 * (spec_norm - spec_min) / (spec_max - spec_min)\n",
        "    spec_scaled = spec_scaled.astype(np.uint8)\n",
        "    \n",
        "    return spec_scaled"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFctlhrhvc1J"
      },
      "source": [
        "## Splitting Val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF5Zd8bZwzR3"
      },
      "source": [
        "# Training an Artificial Neural Network on time-series audio data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCog6GQ0w2jO"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "# Defining dataset batch size\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def load_data(df, id_col, label_col = None, data_path = './', audio_len = 4):\n",
        "    '''\n",
        "    Loads the audio time-series data\n",
        "    \n",
        "    Parameters\n",
        "    -------------\n",
        "    df: The dataframe that contains the file name and corresponding label\n",
        "    \n",
        "    id_col: The column name that contains the file name\n",
        "    \n",
        "    label_col: The column name that contains the label\n",
        "    '''\n",
        "    audio_time_series = []\n",
        "    sample_rates = []\n",
        "    labels = []\n",
        "    \n",
        "    tot = len(df)\n",
        "    curr = 0\n",
        "    \n",
        "    for idx in df.index:\n",
        "        try:\n",
        "            file_name = str(df['filename'][idx]) \n",
        "            \n",
        "            temp = data_path + file_name\n",
        "            wav, sr = librosa.load(str(temp))\n",
        "            \n",
        "            wav = get_fixed_audio_len(wav, sr, audio_len)\n",
        "    \n",
        "            audio_time_series.append(wav)\n",
        "            sample_rates.append(sr)\n",
        "            \n",
        "            if label_col is not None:\n",
        "                labels.append(df[label_col][idx])\n",
        "            \n",
        "            curr += 1\n",
        "            drawProgressBar(curr, tot, barLen = 40)\n",
        "        \n",
        "        except KeyboardInterrupt:\n",
        "            print('KeyBoardInterrupt')\n",
        "            break\n",
        "        \n",
        "        except Exception:\n",
        "            print(\"Couldn't read file\", df[id_col][idx])\n",
        "            curr += 1\n",
        "            \n",
        "    print('\\n')\n",
        "    return np.stack(audio_time_series, axis = 0), np.array(sample_rates), np.array(labels)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_CSV = '/content/SUBESCO/train/train.csv'\n",
        "TEST_CSV = '/content/SUBESCO/test/test.csv'\n",
        "VALID_CSV = '/content/SUBESCO/valid/valid.csv'\n",
        "\n",
        "TRAIN_PATH = '/content/SUBESCO/train/'\n",
        "TEST_PATH = '/content/SUBESCO/test/'\n",
        "VALID_PATH = '/content/SUBESCO/valid/'\n"
      ],
      "metadata": {
        "id": "jQgvh1dPnfK9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fG1bwBbSxEKA",
        "outputId": "f896615f-d23a-4dbe-c4f0-e1ed723f51d4"
      },
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "start_time = time.time()\n",
        "\n",
        "train_time_series, _, train_labels = load_data(pd.read_csv(TRAIN_CSV), 'filename', 'label_id', TRAIN_PATH)\n",
        "\n",
        "val_time_series, _, val_labels = load_data(pd.read_csv(VALID_CSV), 'filename', 'label_id', VALID_PATH)\n",
        "\n",
        "test_time_series, _, test_labels = load_data(pd.read_csv(TEST_CSV), 'filename', 'label_id', TEST_PATH)\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: [========================================] 700/700\n",
            "\n",
            "Progress: [========================================] 1400/1400\n",
            "\n",
            "--- 536.0539813041687 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgB-C1s1sIIZ",
        "outputId": "04b3a497-3003-46d3-c273-ccf25bd395d5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#saving pickle\n",
        "import bz2\n",
        "import pickle\n",
        "\n",
        "dataDic = { 'train_time_series': train_time_series, 'train_labels': train_labels,\n",
        "            'val_time_series': val_time_series, 'val_labels': val_labels,\n",
        "            'test_time_series': test_time_series, 'test_labels': test_labels,}\n",
        "\n",
        "filename = 'dataDic'\n",
        "outfile = open(filename,'wb')\n",
        "\n",
        "pickle.dump(dataDic,outfile)\n",
        "outfile.close()"
      ],
      "metadata": {
        "id": "ptt5ncwco60f"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infile = open(filename,'rb')\n",
        "dataDic = pickle.load(infile)\n",
        "infile.close()"
      ],
      "metadata": {
        "id": "j7qy32Y4qxPF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSpaBwHPHZmJ",
        "outputId": "d53f751d-374c-47c9-a38d-067701f97980"
      },
      "source": [
        "dataDic['train_time_series'].shape,dataDic['val_time_series'].shape,dataDic['test_time_series'].shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4900, 88200), (700, 88200), (1400, 88200))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r '/content/dataDic' /content/gdrive/MyDrive/CSE465"
      ],
      "metadata": {
        "id": "T5fqPbkHsmDO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1ykonyMEqdk"
      },
      "source": [
        "# Convert numpy arrays to torch tensors\n",
        "train_time_series = torch.from_numpy(dataDic['train_time_series'])\n",
        "train_labels = torch.from_numpy(dataDic['train_labels']).long()\n",
        "\n",
        "val_time_series = torch.from_numpy(dataDic['val_time_series'])\n",
        "val_labels = torch.from_numpy(dataDic['val_labels']).long()\n",
        "\n",
        "test_time_series = torch.from_numpy(dataDic['test_time_series'])\n",
        "test_labels = torch.from_numpy(dataDic['test_labels']).long()\n",
        "\n",
        "# Create data loaders\n",
        "train_time_series = data_utils.TensorDataset(train_time_series, train_labels)\n",
        "train_loader = data_utils.DataLoader(train_time_series, batch_size = BATCH_SIZE, shuffle = True)\n",
        "\n",
        "val_time_series = data_utils.TensorDataset(val_time_series, val_labels)\n",
        "val_loader = data_utils.DataLoader(val_time_series, batch_size = BATCH_SIZE, shuffle = True)\n",
        "\n",
        "test_time_series = data_utils.TensorDataset(test_time_series, test_labels)\n",
        "test_loader = data_utils.DataLoader(test_time_series, batch_size = BATCH_SIZE, shuffle = True)\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes ={0: 'ANGRY',\n",
        " 1: 'DISGUST',\n",
        " 2: 'FEAR',\n",
        " 3: 'HAPPY',\n",
        " 4: 'NEUTRAL',\n",
        " 5: 'SAD',\n",
        " 6: 'SURPRISE'}"
      ],
      "metadata": {
        "id": "DFRPUx0FjcD_"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eD1gTmGLH5GQ",
        "outputId": "1758a709-6531-40d8-967e-3abc49d93b0c"
      },
      "source": [
        "# Defining training parameters\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 10\n",
        "NUM_CLASSES = len(classes)\n",
        "\n",
        "N_FEATURES = train_time_series[0][0].shape[0]\n",
        "N_FEATURES"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88200"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmob2OY_QsFA"
      },
      "source": [
        "def evaluate(model, test_loader):\n",
        "    '''\n",
        "    Returns the accuracy and loss of a model\n",
        "    \n",
        "    Parameters\n",
        "    --------------\n",
        "    model: A PyTorch neural network\n",
        "    \n",
        "    test_loader: The test dataset in the form of torch DataLoader\n",
        "    '''\n",
        "    model.eval()\n",
        "    num_test_batches = len(test_loader)\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        total_loss = 0\n",
        "        for i, batch in enumerate(test_loader):\n",
        "            inputs, labels = batch\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, dim = 1)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Printing progress\n",
        "            drawProgressBar((i+1), num_test_batches)\n",
        "        \n",
        "        accuracy = correct/total\n",
        "        test_loss = total_loss/num_test_batches\n",
        "    \n",
        "    return accuracy, test_loss"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iAofwVrSPrI"
      },
      "source": [
        "# Training a Convolutional neural network on spectrogram images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeVdfw3GST4Y"
      },
      "source": [
        "train_sr = 22050\n",
        "val_sr = 22050\n",
        "test_sr = 22050\n",
        "\n",
        "def get_spec_loader(audio_time_series, sr, batch_size, shuffle = False):\n",
        "    '''\n",
        "    Returns data loader of spectrogram images\n",
        "    \n",
        "    Parameters\n",
        "    ------------\n",
        "    audio_time_series: Tensor Dataset with wav, label iterables\n",
        "    \n",
        "    sr: Sample rate\n",
        "    \n",
        "    batch_size: The batch size of data loader\n",
        "    '''\n",
        "    audio_spec_img = []\n",
        "    labels = []\n",
        "    curr = 0\n",
        "    tot = len(audio_time_series)\n",
        "\n",
        "    for wav, label in audio_time_series:\n",
        "        spec_img = spec_to_image(get_melspectrogram_db(wav.numpy(), sr))\n",
        "        spec_img = np.expand_dims(spec_img, axis = 0)\n",
        "        audio_spec_img.append(spec_img)\n",
        "        labels.append(label)\n",
        "\n",
        "        curr += 1\n",
        "        drawProgressBar(curr, tot, barLen = 40)\n",
        "\n",
        "    audio_spec_img = torch.Tensor(audio_spec_img)\n",
        "    audio_spec_img = audio_spec_img / 255\n",
        "    \n",
        "    labels = torch.Tensor(labels).long()\n",
        "\n",
        "    audio_spec_img = data_utils.TensorDataset(audio_spec_img, labels)\n",
        "    audio_loader = data_utils.DataLoader(audio_spec_img, batch_size = batch_size, shuffle = shuffle)\n",
        "    \n",
        "    return audio_loader"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnbWUVyCSaWH",
        "outputId": "d76463ae-c4d5-4f53-8fa2-0b02e8598915"
      },
      "source": [
        "# Getting the spectrogram image for each audio in train set\n",
        "start_time = time.time()\n",
        "train_loader = get_spec_loader(train_time_series, train_sr, BATCH_SIZE, shuffle = True)\n",
        "val_loader = get_spec_loader(val_time_series, val_sr, BATCH_SIZE, shuffle = True)\n",
        "test_loader = get_spec_loader(test_time_series, test_sr, BATCH_SIZE, shuffle = True)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: [========================================] 1400/1400"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlrpLW0wSxQm"
      },
      "source": [
        "## Model cnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4DKAzI_SqAP"
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        \n",
        "        # Layer 1, Input shape (1, 128, 173) ->  Output shape (8, 62, 84)\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = (5, 6)), \n",
        "            nn.ReLU(), \n",
        "            nn.MaxPool2d(kernel_size = (2, 2)))\n",
        "        \n",
        "        # Layer 2, Input shape (8, 62, 84) -> Output shape (16, 30, 41)\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = (3, 3)), \n",
        "            nn.ReLU(), \n",
        "            nn.MaxPool2d(kernel_size = (2, 2)))\n",
        "        \n",
        "        # Layer 3, Input shape (16, 30, 41) -> Output shape (64, 10, 15)\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = (6, 7)), \n",
        "            nn.ReLU(), \n",
        "            nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = (6, 6)), \n",
        "            nn.ReLU(), \n",
        "            nn.MaxPool2d(kernel_size = (2, 2)))\n",
        "        \n",
        "        # Fully Connected layer 1, Input features 64 * 10 * 15 -> Output features 512\n",
        "        self.fc1 = nn.Linear(in_features = 64 * 10 * 15, out_features = 512)\n",
        "        \n",
        "        # Fully Connected layer 2, Input features 512 -> Output features 256\n",
        "        self.fc2 = nn.Linear(in_features = 512, out_features = 256)\n",
        "        \n",
        "        # Fully Connected layer 3, Input features 256 -> Output features 128\n",
        "        self.fc3 = nn.Linear(in_features = 256, out_features = 128)\n",
        "        \n",
        "        # Fully Connected layer 4, Input features 128 -> Output features 7\n",
        "        self.fc4 = nn.Linear(in_features = 128, out_features = NUM_CLASSES)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        \n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]\n",
        "        n_features = 1\n",
        "        for s in size:\n",
        "            n_features = n_features * s\n",
        "        \n",
        "        return n_features"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69uM_9H3TjSm"
      },
      "source": [
        "# Defining loss and optimizer\n",
        "NUM_CLASSES = len(classes)\n",
        "model = ConvNet().to(device)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "BATCH_SIZE = 32\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "#step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)"
      ],
      "metadata": {
        "id": "gEhaMKMxvJIn"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    size = len(dataloader.dataset)  \n",
        "    for batch, (X,y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        \n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            \n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    return model\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for (X,y) in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "-Bfb2uxcoy2I"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "EPOCHS = 30\n",
        "start = time.time()\n",
        "for t in range(EPOCHS):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    model = train_loop(train_loader, model, criterion, optimizer)\n",
        "    test_loop(val_loader, model, criterion)\n",
        "final = (time.time() - start)/60\n",
        "print(f\"Done for all {EPOCHS} epochs in {math.ceil(final)} minutes\\n\")"
      ],
      "metadata": {
        "id": "bN0fMA_Xo_1l",
        "outputId": "ad6f87d8-5ef7-4ecc-a64e-44cc5a587e0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.851983  [    0/ 4900]\n",
            "loss: 0.469104  [ 1600/ 4900]\n",
            "loss: 0.816316  [ 3200/ 4900]\n",
            "loss: 0.833359  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 44.3%, Avg loss: 2.133436 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.912570  [    0/ 4900]\n",
            "loss: 0.682967  [ 1600/ 4900]\n",
            "loss: 0.753245  [ 3200/ 4900]\n",
            "loss: 0.745694  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 43.4%, Avg loss: 2.028360 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.703111  [    0/ 4900]\n",
            "loss: 0.940002  [ 1600/ 4900]\n",
            "loss: 0.517040  [ 3200/ 4900]\n",
            "loss: 0.599516  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 45.4%, Avg loss: 2.402766 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.878561  [    0/ 4900]\n",
            "loss: 0.569081  [ 1600/ 4900]\n",
            "loss: 0.558859  [ 3200/ 4900]\n",
            "loss: 0.492488  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 46.3%, Avg loss: 2.103584 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.671384  [    0/ 4900]\n",
            "loss: 0.616357  [ 1600/ 4900]\n",
            "loss: 0.712727  [ 3200/ 4900]\n",
            "loss: 0.846066  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 43.7%, Avg loss: 2.410116 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.592108  [    0/ 4900]\n",
            "loss: 0.644319  [ 1600/ 4900]\n",
            "loss: 0.579777  [ 3200/ 4900]\n",
            "loss: 0.875881  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 41.3%, Avg loss: 2.655668 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.468994  [    0/ 4900]\n",
            "loss: 0.734010  [ 1600/ 4900]\n",
            "loss: 0.437776  [ 3200/ 4900]\n",
            "loss: 0.631438  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 43.7%, Avg loss: 2.286908 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.466475  [    0/ 4900]\n",
            "loss: 0.412247  [ 1600/ 4900]\n",
            "loss: 0.557357  [ 3200/ 4900]\n",
            "loss: 0.847435  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 43.9%, Avg loss: 2.420159 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.468491  [    0/ 4900]\n",
            "loss: 0.373527  [ 1600/ 4900]\n",
            "loss: 0.862219  [ 3200/ 4900]\n",
            "loss: 0.763082  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 46.0%, Avg loss: 2.259184 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.315285  [    0/ 4900]\n",
            "loss: 0.538697  [ 1600/ 4900]\n",
            "loss: 0.545521  [ 3200/ 4900]\n",
            "loss: 0.443447  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 41.9%, Avg loss: 2.809727 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.549672  [    0/ 4900]\n",
            "loss: 0.294458  [ 1600/ 4900]\n",
            "loss: 0.412218  [ 3200/ 4900]\n",
            "loss: 0.325378  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 42.6%, Avg loss: 3.037700 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.308375  [    0/ 4900]\n",
            "loss: 0.502775  [ 1600/ 4900]\n",
            "loss: 0.554299  [ 3200/ 4900]\n",
            "loss: 0.524411  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 47.0%, Avg loss: 2.512981 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.667401  [    0/ 4900]\n",
            "loss: 0.532637  [ 1600/ 4900]\n",
            "loss: 0.403334  [ 3200/ 4900]\n",
            "loss: 0.557440  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 38.1%, Avg loss: 3.292042 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.452084  [    0/ 4900]\n",
            "loss: 0.437248  [ 1600/ 4900]\n",
            "loss: 0.595854  [ 3200/ 4900]\n",
            "loss: 0.313446  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 45.0%, Avg loss: 2.710557 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.543795  [    0/ 4900]\n",
            "loss: 0.686239  [ 1600/ 4900]\n",
            "loss: 0.440275  [ 3200/ 4900]\n",
            "loss: 0.631749  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 42.0%, Avg loss: 3.192485 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.548407  [    0/ 4900]\n",
            "loss: 0.303342  [ 1600/ 4900]\n",
            "loss: 0.396529  [ 3200/ 4900]\n",
            "loss: 0.348676  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 43.7%, Avg loss: 2.901256 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.614861  [    0/ 4900]\n",
            "loss: 0.110304  [ 1600/ 4900]\n",
            "loss: 0.432505  [ 3200/ 4900]\n",
            "loss: 0.540196  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 40.6%, Avg loss: 3.483431 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.315401  [    0/ 4900]\n",
            "loss: 0.474006  [ 1600/ 4900]\n",
            "loss: 0.416686  [ 3200/ 4900]\n",
            "loss: 0.463567  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 40.4%, Avg loss: 3.347796 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.308213  [    0/ 4900]\n",
            "loss: 0.437402  [ 1600/ 4900]\n",
            "loss: 0.350743  [ 3200/ 4900]\n",
            "loss: 0.367128  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 42.4%, Avg loss: 3.441026 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.257456  [    0/ 4900]\n",
            "loss: 0.233503  [ 1600/ 4900]\n",
            "loss: 0.257076  [ 3200/ 4900]\n",
            "loss: 0.343527  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 47.3%, Avg loss: 3.073675 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.215488  [    0/ 4900]\n",
            "loss: 0.567554  [ 1600/ 4900]\n",
            "loss: 0.323990  [ 3200/ 4900]\n",
            "loss: 0.245023  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 43.4%, Avg loss: 3.507083 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.372140  [    0/ 4900]\n",
            "loss: 0.348469  [ 1600/ 4900]\n",
            "loss: 0.176466  [ 3200/ 4900]\n",
            "loss: 0.209670  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 39.9%, Avg loss: 4.210981 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.270234  [    0/ 4900]\n",
            "loss: 0.332096  [ 1600/ 4900]\n",
            "loss: 0.418898  [ 3200/ 4900]\n",
            "loss: 0.306098  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 43.1%, Avg loss: 3.569514 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.203009  [    0/ 4900]\n",
            "loss: 0.198549  [ 1600/ 4900]\n",
            "loss: 0.286687  [ 3200/ 4900]\n",
            "loss: 0.392628  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 44.9%, Avg loss: 3.688554 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.186037  [    0/ 4900]\n",
            "loss: 0.187585  [ 1600/ 4900]\n",
            "loss: 0.240538  [ 3200/ 4900]\n",
            "loss: 0.271694  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 41.1%, Avg loss: 4.295568 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.243018  [    0/ 4900]\n",
            "loss: 0.193095  [ 1600/ 4900]\n",
            "loss: 0.388969  [ 3200/ 4900]\n",
            "loss: 0.229490  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 43.9%, Avg loss: 4.330341 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.103056  [    0/ 4900]\n",
            "loss: 0.238575  [ 1600/ 4900]\n",
            "loss: 0.243692  [ 3200/ 4900]\n",
            "loss: 0.267512  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 46.3%, Avg loss: 4.065084 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.173288  [    0/ 4900]\n",
            "loss: 0.303742  [ 1600/ 4900]\n",
            "loss: 0.326098  [ 3200/ 4900]\n",
            "loss: 0.117709  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 42.4%, Avg loss: 4.388835 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.143913  [    0/ 4900]\n",
            "loss: 0.183735  [ 1600/ 4900]\n",
            "loss: 0.244824  [ 3200/ 4900]\n",
            "loss: 0.159618  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 44.1%, Avg loss: 4.525946 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.242753  [    0/ 4900]\n",
            "loss: 0.095222  [ 1600/ 4900]\n",
            "loss: 0.116000  [ 3200/ 4900]\n",
            "loss: 0.108539  [ 4800/ 4900]\n",
            "Test Error: \n",
            " Accuracy: 45.9%, Avg loss: 4.241723 \n",
            "\n",
            "Done for all 30 epochs in 3 minutes\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_single_epoch(model, dataloader, device):\n",
        "  correct = 0\n",
        "  size = len(dataloader.dataset)\n",
        "\n",
        "  model.eval()\n",
        "  for input,target in dataloader:\n",
        "        input, target = input.to(device), target.to(device)\n",
        "        # calculate loss\n",
        "        prediction = model(input)\n",
        "        correct += (prediction.argmax(1) == target).type(torch.float).sum().item()\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}% \\n\")"
      ],
      "metadata": {
        "id": "liW_rSOrpeEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_single_epoch(model,test_loader,device)"
      ],
      "metadata": {
        "id": "-IkPWG_BpjLW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}